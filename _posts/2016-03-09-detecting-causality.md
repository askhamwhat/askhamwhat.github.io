---
layout: post
title: "Journal club: Detecting causality in complex ecosystems"
date: 2016-03-09
---

For the [Kutz research group journal club](http://faculty.washington.edu/kutz/page5/page20/),
I prepared some notes on the paper "Detecting causality in complex 
ecosystems" by Sugihara et al. The paper and supplementary materials 
can be found at the [Science magazine website](http://science.sciencemag.org/content/338/6106/496).
For a *Science* article, it is remarkably clear and informative, 
perfectly capable of speaking for itself. I recommend the original article and
supplementary materials for a deeper understanding of the 
convergent cross mapping (CCM) methodology. The Sugihara 
group also has a number of YouTube videos on the topic, which are
linked to below. My goal here is to outline the main ideas
from my perspective and to offer my take-aways. I also provide 
some simple MatLab scripts which replicate experiments from the article.

#### Setting

> Correlation does not necessarily imply 
> causation. 

The authors of "Detecting causality ..." credit
this maxim to Bishop Berkeley and his *A Treatise Concerning
the Principles of Human Knowledge* --- it is an old
and generally well-accepted notion. Indeed, it is rather 
straightforward to find evidence of the
danger in linking correlation to causation. 
Despite the high correlation coefficient, few would claim
that the divorce rate in Maine *causes* per capita margarine
consumption. For more examples, see the strongest 
modern proponent of the maxim, [Tyler Vigen](http://tylervigen.com/spurious-correlations).

It turns out that the relationship between correlation and 
causation is particularly nebulous in the dynamical
systems setting. To be concrete, we say that two variables are 
causally linked if they come from the same dynamical system
(we will discuss one-way causation further down). 
Consider the dynamical system given by 

$$ \begin{array}{rl} X(t+1) &= X(t) ( r_x - r_x X(t) - \beta_{xy} Y(t)) \\
   Y(t+1) &= Y(t) ( r_y - r_y Y(t) - \beta_{yx} X(t)) \end{array} . $$

The variables \\(X\\) and \\(Y\\) are causally linked when \\(\beta_{xy} > 0\\) 
and \\( \beta_{yx} > 0 \\). But for a long enough simulation, it is possible 
to find time windows where this system appears correlated, anti-correlated, and
uncorrelated. The image below shows example windows of 
length 20 which are generated by a single run of the
system (the first plot shows the correlation coefficient
for a window of length 20 as you vary the starting point).

<img src="/post_resources/images/correlation-mirage.png" 
     alt="correlation mirage" width="600" />

The code used to generate this plot can be found
[here](/post_resources/code/fig1code.m). The phenomena
shown above are referred to as "mirage correlations"
and demonstrate that the correlation coefficient is not
related at all to causation for such systems. 
For dynamical systems, even the lack of correlation 
does not imply a lack of causation.

Fortunately, correlation is not the only known
test for cause. The concept of 
[Granger causality](https://en.wikipedia.org/wiki/Granger_causality)
comes from the field of economics and is a test of 
"predictive causality" for stochastic systems.
In brief, a variable \\(X\\) is said to Granger
cause \\(Y\\) if the prior information of 
\\(X\\) improves predictions of the future behavior
of \\(Y\\). This has proved a powerful tool for
stochastic systems and earned Granger a Nobel prize.
Unfortunately, Granger causality is
quite distinct from the notion of causality 
that is natural in the dynamical systems setting. Granger
causality assumes that the cause contains unique
information about the future state of the effect,
which is not true of variables from the 
same dynamical system.
In the supplementary materials, Sugihara et al. 
demonstrate that the tools of Granger
causality give erroneous results when applied
to the examples of the main paper.
As an alternative, the authors introduce
convergent cross mapping (CCM), a
framework for determining causality in dynamical
systems.

#### CCM Theory

The theoretical foundation of CCM is [Takens'
theorem](https://en.wikipedia.org/wiki/Takens%27_theorem).
The result is rather technical but implies that
the time series for one observed variable of the
dynamical system can be used to reconstruct
essential properties of the original system. For 
CCM, this is accomplished by contructing a "shadow
manifold" using the time series of an observed
variable. The shadow manifold is given by 
vectors of the form \\(\vec{X}(t) = (X(t),X(t-\tau),
\ldots , X(t-(m-1)\tau)) \\), consisting of
lagged copies of \\(X\\). For sufficiently
large \\(m\\), the shadow manifold is diffeomorphic
to the strange attractor of the original system
(supposing that the dynamics live on a strange attractor).
These ideas are illustrated in the video below.

<iframe title="YouTube video player" width="480" height="390"
src = "http://www.youtube.com/v/rs3gYeZeJcw" frameborder="0"
allowfullscreen></iframe>

If two variables \\(X\\) and \\(Y\\) both come
from the same dynamical system, then their corresponding
shadow manifolds should be diffeomorphic (there are some 
exceptions to this). Therefore, nearby points on the
\\(X\\) shadow manifold should correspond to 
nearby points on the \\(Y\\) shadow manifold. CCM
is based on using this property to make predictions
of the state of \\(Y\\) using the shadow manifold of \\(X\\)
and vice-versa. A nice feature of CCM is that, with
more and more observations of the system, the
shadow manifolds become better resolved and the prediction
generated from this procedure would become more 
accurate in the case that \\(X\\)
and \\(Y\\) are causally linked. The following video
demonstrates these ideas.

<iframe title="YouTube video player" width="480" height="390"
src = "http://www.youtube.com/v/NrFdIz-D2yM" frameborder="0"
allowfullscreen></iframe>

#### CCM Algorithm

Much of the analysis in the CCM framework is based on
the ability to use the shadow manifold of one variable
to make a prediction of another. The quality of this 
prediction can then be used as the basis for a number
of tests. We outline the procedure of producing an approximation
to \\(Y(t)\\) from the shadow manifold of \\(X\\).
Let \\( \{ X(1), \ldots, X(L) \} \\) and 
\\( \{ Y(1), \ldots, Y(L) \} \\) be given. 

1. Construct a shadow manifold of dimension \\(m\\) by simply
   forming the set 

   $$ M_X = \{ \vec{X}(t) = (X(t),X(t-1),\ldots,X(t-(m-1)) ): t = m, \ldots, L \} . $$

2. For a given time $t$, compute the $m+1$ nearest neighbors
   of \\(\vec{X}(t)\\) (excluding \\(\vec{X}(t)\\) itself). Let
   the times corresponding to these points be \\(t_1, \ldots, t_{m+1} \\).
   
3. Compute the approximation \\(\hat{Y}(t) | M_X\\) as a weighted sum
   of the \\(Y(t_1),\ldots, Y(t_{m+1})\\). 

The paper calls for a specific weighting to be used
(see the supplement for details) but I suspect other weightings
may work as well. The details of how to choose an appropriate
dimension \\(m\\) for the shadow manifold are largely left out.
For the examples, it seems that the dimension was determined by 
starting with some initial guess and increasing until the predictions
stopped improving (the Whitney embedding theorem provides an
upper bound for this process).

#### CCM Style Analysis --- Synthetic Data

#### CCM Style Analysis --- Real Data


